{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVsfg8weBKcx"
   },
   "source": [
    "## TFRecords creation\n",
    "The goal of this Notebook is to **convert all the images that have been preprocessed in the Notebook** ***2_MRI_preprocessing*** **and saved as a numpy’s compressed format (.npz) in the folder 'Datasets/Image_files' to TFRecords.**\n",
    "\n",
    "According to [Tensorflow](https://www.tensorflow.org/tutorials/load_data/tfrecord), TFRecords are used to store the data as a sequence of binary strings. The main advantage of using TFRecords is that it speeds up data reading.\n",
    "\n",
    "This notebook is structured as follows:\n",
    "   - Initial set-up\n",
    "   - Import libraries\n",
    "   - Load features\n",
    "   - Load labels\n",
    "   - Create TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu6CFWX7BKc1"
   },
   "source": [
    "### Initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErrixVFNBKc2"
   },
   "source": [
    "#### Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1626803093217,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "v7ywVwLNBKc3"
   },
   "outputs": [],
   "source": [
    "# Specify if user is working on Google Drive\n",
    "google_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43050,
     "status": "ok",
     "timestamp": 1626803167832,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "R87yUdRcBKc4",
    "outputId": "45b1109d-5b7c-45c5-9115-310c32dde9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "if google_colab == True:\n",
    "    \n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    path = './drive/MyDrive/TFM/Code/'\n",
    "    \n",
    "    import os\n",
    "    os.chdir(path)\n",
    "\n",
    "else:\n",
    "    path = '../'\n",
    "    \n",
    "    import os\n",
    "    os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F61-09-wBKc5"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4486,
     "status": "ok",
     "timestamp": 1626803178408,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "krF9gmZWBKc6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcYUQqsmBKc7"
   },
   "source": [
    "### Load features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3To2W-WBKc8"
   },
   "source": [
    "#### Load 3D images directories (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1626803230440,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "FKdbRlNTBKc8"
   },
   "outputs": [],
   "source": [
    "# Specify folder where there are the 3D images in numpy’s compressed format (.npz)\n",
    "directory_images = './Datasets/Image_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8888,
     "status": "ok",
     "timestamp": 1626803241062,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "aSetKI2EBKc9",
    "outputId": "17169aac-042d-418b-e4d0-af446b31943d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of 3D images: 1146\n"
     ]
    }
   ],
   "source": [
    "# Get list of filenames from 3D volumes\n",
    "filenames = os.listdir(directory_images)\n",
    "\n",
    "# Remove \".DS_Store\" file from list\n",
    "if '.DS_Store' in filenames:\n",
    "    filenames.remove('.DS_Store')\n",
    "\n",
    "# Include all the path for each file\n",
    "filenames = [directory_images + file for file in filenames]\n",
    "\n",
    "# Check number of images loaded\n",
    "print('[+] Number of 3D images:', len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1626803252242,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "4lWlKHh4BKc_"
   },
   "outputs": [],
   "source": [
    "# Shuffle list of filenames\n",
    "random.shuffle(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM6ummVvBKdA"
   },
   "source": [
    "#### Split dataset into training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiN9jZnIBKdA"
   },
   "outputs": [],
   "source": [
    "train_size = 0.7\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ACqUkmmBKdB"
   },
   "outputs": [],
   "source": [
    "train_filenames, test_filenames = train_test_split(filenames, \n",
    "                                               test_size = 0.3, \n",
    "                                               random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62fW73hCBKdC"
   },
   "outputs": [],
   "source": [
    "test_filenames, val_filenames = train_test_split(test_filenames, \n",
    "                                             test_size = 0.5, \n",
    "                                             random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NotWZBXmBKdC",
    "outputId": "34cd3536-467e-4fb8-f894-5d19454c17d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Training size: 802\n",
      "[+] Validation size: 172\n",
      "[+] Testing size: 172\n"
     ]
    }
   ],
   "source": [
    "# Check size of each dataset\n",
    "print(f'[+] Training size:', len(train_filenames))\n",
    "print(f'[+] Validation size:', len(val_filenames))\n",
    "print(f'[+] Testing size:', len(test_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFwzj7U-BKdD"
   },
   "source": [
    "#### Load array for each 3D image (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_33i81QBKdE"
   },
   "outputs": [],
   "source": [
    "def return_volumes(filenames):\n",
    "    '''\n",
    "    Function used to load numpy arrays (.npz) from a list of directories\n",
    "    Input: directories where there are the numpy arrays\n",
    "    Output: array with 3D images\n",
    "    ''' \n",
    "\n",
    "    # List where to save the 3D images\n",
    "    volumes = []\n",
    "\n",
    "    # Load 3D images from filenames list\n",
    "    for file in filenames:\n",
    "\n",
    "        # Read 3D image\n",
    "        volume = np.load(file, allow_pickle= True)['arr_0']\n",
    "\n",
    "        # Append 3D image to volumes list \n",
    "        volumes.append(volume)\n",
    "        \n",
    "        if len(volumes) % 100 == 0:\n",
    "            print('[+] Number of images loaded:', len(volumes))\n",
    "    \n",
    "    print('Total number of images loaded:', len(volumes))\n",
    "        \n",
    "    return np.array(volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POJWJUn4BKdE",
    "outputId": "c7f0cbaa-0806-40d2-b3e1-2bb3fc76bfac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of images loaded: 100\n",
      "[+] Number of images loaded: 200\n",
      "[+] Number of images loaded: 300\n",
      "[+] Number of images loaded: 400\n",
      "[+] Number of images loaded: 500\n",
      "[+] Number of images loaded: 600\n",
      "[+] Number of images loaded: 700\n",
      "[+] Number of images loaded: 800\n",
      "Total number of images loaded: 802\n"
     ]
    }
   ],
   "source": [
    "# Load training images\n",
    "train_dataset = return_volumes(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT8V31ZhBKdF",
    "outputId": "215c374f-65c0-460c-91ef-dc5183230238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of images loaded: 100\n",
      "Total number of images loaded: 172\n"
     ]
    }
   ],
   "source": [
    "# Load validation images\n",
    "val_dataset = return_volumes(val_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj9ItHizBKdG",
    "outputId": "b8344f7c-842d-41d1-b971-6c0f9484e232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of images loaded: 100\n",
      "Total number of images loaded: 172\n"
     ]
    }
   ],
   "source": [
    "# Load testing images\n",
    "test_dataset = return_volumes(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YO8ihdVlBKdH",
    "outputId": "ad6e3031-1eff-4a7a-ae1a-e95b6a888897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Training shape: (802, 110, 130, 80)\n",
      "[+] Validation shape: (172, 110, 130, 80)\n",
      "[+] Testing shape: (172, 110, 130, 80)\n"
     ]
    }
   ],
   "source": [
    "# Check size of each dataset\n",
    "print(f'[+] Training shape:', train_dataset.shape)\n",
    "print(f'[+] Validation shape:', val_dataset.shape)\n",
    "print(f'[+] Testing shape:', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcoky-wzBKdH"
   },
   "source": [
    "### Load labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPHuLNrIBKdI"
   },
   "source": [
    "#### Load CSV files with image details: images IDs and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 957,
     "status": "ok",
     "timestamp": 1626803284000,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "R6DWARq7BKdJ",
    "outputId": "92883b0f-5bb7-4239-ee79-6bd017bdd8b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageDataID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>AcqDate</th>\n",
       "      <th>Format</th>\n",
       "      <th>Downloaded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I125941</td>\n",
       "      <td>137_S_1426</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>10/30/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I121703</td>\n",
       "      <td>128_S_1408</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/19/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I121637</td>\n",
       "      <td>037_S_1421</td>\n",
       "      <td>MCI</td>\n",
       "      <td>F</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/17/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I122382</td>\n",
       "      <td>128_S_1407</td>\n",
       "      <td>MCI</td>\n",
       "      <td>F</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/05/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I121689</td>\n",
       "      <td>127_S_1427</td>\n",
       "      <td>MCI</td>\n",
       "      <td>F</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/02/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ImageDataID     Subject Group Sex  ...       Type     AcqDate Format Downloaded\n",
       "0     I125941  137_S_1426   MCI   M  ...  Processed  10/30/2008  NiFTI        NaN\n",
       "1     I121703  128_S_1408   MCI   M  ...  Processed   9/19/2008  NiFTI        NaN\n",
       "2     I121637  037_S_1421   MCI   F  ...  Processed   9/17/2008  NiFTI        NaN\n",
       "3     I122382  128_S_1407   MCI   F  ...  Processed   9/05/2008  NiFTI        NaN\n",
       "4     I121689  127_S_1427   MCI   F  ...  Processed   9/02/2008  NiFTI        NaN\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load individuals CSV files with image details\n",
    "df_1 = pd.read_csv('./Datasets/ADNI1_Complete_1Yr_1.5T.csv')\n",
    "df_2 = pd.read_csv('./Datasets/ADNI1_Complete_2Yr_1.5T.csv')\n",
    "df_3 = pd.read_csv('./Datasets/ADNI1_Complete_3Yr_1.5T.csv')\n",
    "\n",
    "# Concatenate all CSV files in a unique dataframe\n",
    "df = pd.concat([df_1, df_2, df_3])\n",
    "\n",
    "# Remove extra whitespaces from column names\n",
    "df.columns = df.columns.str.replace(\" \", \"\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiQ0nPuaBKdK",
    "outputId": "818ff451-bcc9-4b3b-e716-f050bea31f47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageDataID</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I125941</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I121703</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I121637</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I122382</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I121689</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ImageDataID Group\n",
       "0     I125941   MCI\n",
       "1     I121703   MCI\n",
       "2     I121637   MCI\n",
       "3     I122382   MCI\n",
       "4     I121689   MCI"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve only the image ID and Group (class) columns\n",
    "df = df[['ImageDataID', 'Group']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIHx84pBBKdL"
   },
   "source": [
    "#### Load class for each 3D volume (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHvyS1h4BKdM"
   },
   "outputs": [],
   "source": [
    "def return_labels(filenames, df):\n",
    "    '''\n",
    "    Function used to retrieve label for each sample of a dataset\n",
    "    Input: directories where there are the images\n",
    "    Output: array with labels\n",
    "    ''' \n",
    "\n",
    "    # List where to save the class for each image: 0 (CN), 1 (AD)\n",
    "    labels = []\n",
    "\n",
    "    # Load labels from filenames list\n",
    "    for file in filenames:\n",
    "\n",
    "        # Get image ID from file name\n",
    "        image_id = file.split('/')[-1].split('.')[0]\n",
    "\n",
    "        # Retrieve class from dataframe searching by the image ID\n",
    "        label = df['Group'].loc[df['ImageDataID'] == image_id].values[0]\n",
    "\n",
    "        # Assign a class numerical value depending on the group: 0 (CN) or 1 (AD)\n",
    "        if label in ['CN']:\n",
    "            labels.append([0]) \n",
    "        elif label in ['AD']:\n",
    "            labels.append([1])  \n",
    "        else:\n",
    "            print(f'ERROR with image ID {image_id}')\n",
    "    \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxwIpSKhBKdN"
   },
   "outputs": [],
   "source": [
    "# Load training labels\n",
    "train_labels = return_labels(train_filenames, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yReDd7rLBKdO"
   },
   "outputs": [],
   "source": [
    "# Load validation labels\n",
    "val_labels = return_labels(val_filenames, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VflOXS8JBKdP"
   },
   "outputs": [],
   "source": [
    "# Load testing labels\n",
    "test_labels = return_labels(test_filenames, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQq7hnJyBKdQ",
    "outputId": "f85443aa-b4de-458d-c8f2-1948c9c012b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Training labels shape: (802, 1)\n",
      "[+] Validation labels shape: (172, 1)\n",
      "[+] Testing labels shape: (172, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check size of each dataset\n",
    "print(f'[+] Training labels shape:', train_labels.shape)\n",
    "print(f'[+] Validation labels shape:', val_labels.shape)\n",
    "print(f'[+] Testing labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3rJBAogBKdT"
   },
   "source": [
    "### Create TFRecords\n",
    "The following functions have been already defined by Tensorflow, and can be found in this [link](https://www.tensorflow.org/tutorials/load_data/tfrecord)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NW5XD_bBKdV"
   },
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72L3UmBZBKdX"
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    '''\n",
    "    Returns a bytes_list from a string / byte.\n",
    "    '''\n",
    "    \n",
    "    if isinstance(value, type(tf.constant(0))): # if value is tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    "    \n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    '''\n",
    "    Returns a floast_list from a float / double.\n",
    "    '''\n",
    "    \n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    '''\n",
    "    Returns an int64_list from a bool / enum / int / uint.\n",
    "    '''\n",
    "    \n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_array(array):\n",
    "    \n",
    "    array = tf.io.serialize_tensor(array)\n",
    "    \n",
    "    return array\n",
    "\n",
    "def parse_single_volume(volume, label):\n",
    "    \n",
    "    # Get first value of label, as it is an array of length 1\n",
    "    label = label[0]\n",
    "    \n",
    "    # Define the dictionary -- the structure -- of our single example\n",
    "    data = {'height' : _int64_feature(volume.shape[0]),\n",
    "            'width' : _int64_feature(volume.shape[1]),\n",
    "            'depth' : _int64_feature(volume.shape[2]),\n",
    "            'raw_image' : _bytes_feature(serialize_array(volume)),\n",
    "            'label' : _int64_feature(label)}\n",
    "    \n",
    "    # Create an Example, wrapping the single features\n",
    "    out = tf.train.Example(features = tf.train.Features(feature = data))\n",
    "\n",
    "    return out\n",
    "\n",
    "def write_images_to_tfr(volumes, labels, filename = 'images', max_files = 10, out_dir = './Datasets/TFRecords/'):\n",
    "\n",
    "    # Determine the number of TFRecords needed\n",
    "    splits = (len(volumes)//max_files) + 1 \n",
    "    if len(volumes) % max_files == 0:\n",
    "        splits-=1   \n",
    "    \n",
    "    print(f'[+] Number of TFRecords needed for {len(volumes)} volumes: {splits}')\n",
    "    print(f'    [-] Number of files per TFRecord: {max_files}')\n",
    "    \n",
    "    # Check if the output directory exists\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)   \n",
    "    print(f'\\n[+] Output directory: {out_dir}\\n')\n",
    "    \n",
    "    # Write TFRecords\n",
    "    file_count = 0\n",
    "    \n",
    "    for i in tqdm.tqdm(range(splits)):\n",
    "        \n",
    "        # Retrieve name of the TFRecord\n",
    "        tfr_name = '{}{}_{}.tfrecords'.format(out_dir, i+1, filename)\n",
    "        print(f'[+] Writing TFRecord: {tfr_name}')\n",
    "\n",
    "        # Start writer\n",
    "        writer = tf.io.TFRecordWriter(tfr_name)\n",
    "        current_tfr_count = 0\n",
    "    \n",
    "        while current_tfr_count < max_files: \n",
    "            \n",
    "            # Get the index of the file that we want to parse now\n",
    "            index = i * max_files + current_tfr_count\n",
    "            \n",
    "            # Check if all dataset has been added to TFRecords\n",
    "            if index == len(volumes):\n",
    "                break\n",
    "                \n",
    "            # Retrieve volume and label\n",
    "            current_volume = volumes[index]\n",
    "            current_label = labels[index]\n",
    "\n",
    "            # Create the required example representation\n",
    "            out = parse_single_volume(volume = current_volume, label = current_label)\n",
    "\n",
    "            writer.write(out.SerializeToString())\n",
    "            \n",
    "            # Update counters\n",
    "            current_tfr_count+=1\n",
    "            file_count += 1\n",
    "       \n",
    "        # Close writer\n",
    "        writer.close()\n",
    "    \n",
    "    print(f'Number of files wrote to TFRecords: {file_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpV_Che9BKdY"
   },
   "source": [
    "#### Create training TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIZNxmpKBKdZ",
    "outputId": "4ad6f9b1-498e-48f1-821c-2d071a32b08e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of TFRecords needed for 802 volumes: 27\n",
      "    [-] Number of files per TFRecord: 30\n",
      "\n",
      "[+] Output directory: ../Datasets/TFRecords/Train/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/1_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▎         | 1/27 [00:01<00:40,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/2_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/27 [00:03<00:38,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/3_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 3/27 [00:05<00:43,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/4_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▍        | 4/27 [00:07<00:42,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/5_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▊        | 5/27 [00:09<00:45,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/6_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 6/27 [00:13<00:55,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/7_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 7/27 [00:15<00:51,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/8_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██▉       | 8/27 [00:17<00:44,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/9_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 9/27 [00:19<00:40,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/10_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 10/27 [00:21<00:38,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/11_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 11/27 [00:23<00:33,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/12_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 12/27 [00:25<00:30,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/13_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 13/27 [00:27<00:29,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/14_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 14/27 [00:30<00:30,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/15_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 15/27 [00:32<00:24,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/16_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 16/27 [00:33<00:20,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/17_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 17/27 [00:35<00:17,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/18_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 18/27 [00:36<00:15,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/19_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 19/27 [00:38<00:13,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/20_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 20/27 [00:39<00:11,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/21_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 21/27 [00:41<00:09,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/22_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████▏ | 22/27 [00:42<00:07,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/23_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 23/27 [00:44<00:06,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/24_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 24/27 [00:46<00:04,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/25_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 25/27 [00:47<00:03,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/26_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▋| 26/27 [00:49<00:01,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/27_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:50<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files wrote to TFRecords: 802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_images_to_tfr(train_dataset, train_labels, \n",
    "                    max_files = 30, \n",
    "                    filename = 'train_volumes',\n",
    "                    out_dir = './Datasets/TFRecords/Train/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qy_SRGXBKda"
   },
   "source": [
    "#### Create validation TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQiLA5pkBKdb",
    "outputId": "dc340f3b-17fd-4a3c-c5f8-a11f2adbf5f6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of TFRecords needed for 172 volumes: 6\n",
      "    [-] Number of files per TFRecord: 30\n",
      "\n",
      "[+] Output directory: ../Datasets/TFRecords/Validation/\n",
      "\n",
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/1_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 1/6 [00:01<00:06,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/2_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:02<00:05,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/3_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:04<00:04,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/4_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:06<00:03,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/5_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:07<00:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/6_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files wrote to TFRecords: 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_images_to_tfr(val_dataset, val_labels, \n",
    "                    max_files = 30, \n",
    "                    filename = 'val_volumes',\n",
    "                    out_dir = './Datasets/TFRecords/Validation/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exqrKsF6BKdc"
   },
   "source": [
    "#### Create testing TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViV6lKTkBKdd",
    "outputId": "5ec29e2b-330e-4a72-864c-b715ffeb2329",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of TFRecords needed for 172 volumes: 6\n",
      "    [-] Number of files per TFRecord: 30\n",
      "\n",
      "[+] Output directory: ../Datasets/TFRecords/Test/\n",
      "\n",
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/1_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 1/6 [00:01<00:07,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/2_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:03<00:06,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/3_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:04<00:04,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/4_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:06<00:03,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/5_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:08<00:01,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/6_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:09<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files wrote to TFRecords: 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_images_to_tfr(test_dataset, test_labels, \n",
    "                    max_files = 30, \n",
    "                    filename = 'test_volumes',\n",
    "                    out_dir = './Datasets/TFRecords/Test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2GYKSG2BKde"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_TFR_creation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfm_env",
   "language": "python",
   "name": "tfm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
