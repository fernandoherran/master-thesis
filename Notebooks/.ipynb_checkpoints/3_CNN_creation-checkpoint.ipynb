{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgFS5tRszcAb"
   },
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kcyFTQGzcAg"
   },
   "source": [
    "**Google Colab**  \n",
    "[TPUs in Colab](https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=kvPXiovhi3ZZ)  \n",
    "[TFRecord](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/tfrecord.ipynb)  \n",
    "\n",
    "**CNN**  \n",
    "[CNN: Beginner guide](https://towardsdatascience.com/convolution-neural-networks-a-beginners-guide-implementing-a-mnist-hand-written-digit-8aa60330d022)  \n",
    "[CNN: Image sequence](https://medium.com/smileinnovation/training-neural-network-with-image-sequence-an-example-with-video-as-input-c3407f7a0b0f)  \n",
    "[CNN: Keras 3D v1](https://keras.io/examples/vision/3D_image_classification/)  \n",
    "[CNN: Keras 3D v2](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)  \n",
    "[CNN: Conv](https://stackoverflow.com/questions/66220774/difference-between-the-input-shape-for-a-1d-cnn-2d-cnn-and-3d-cnn)  \n",
    "[CNN: Arquitecture](https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb)  \n",
    "[CNN: Tutorial](https://jhui.github.io/2017/03/16/CNN-Convolutional-neural-network/)  \n",
    "[CNN: Time distributed](https://medium.com/smileinnovation/how-to-work-with-time-distributed-data-in-a-neural-network-b8b39aa4ce00)  \n",
    "[CNN: ConvLSTM](https://medium.com/neuronio/an-introduction-to-convlstm-55c9025563a7)  \n",
    "[CNN: Course](https://cs231n.github.io/neural-networks-3/#loss)  \n",
    "[Metrics functions](https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)  \n",
    "[Pixels Preprocessing v1](https://machinelearningmastery.com/how-to-normalize-center-and-standardize-images-with-the-imagedatagenerator-in-keras/)  \n",
    "[Pixels Preprocessing v2](https://machinelearningmastery.com/how-to-manually-scale-image-pixel-data-for-deep-learning/)  \n",
    "[Pixels Normalize](https://www.researchgate.net/post/How_do_I_normalize_2_grayscale_images_so_that_they_are_equivalent_to_each_other)  \n",
    "[Epoch vs Batch](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)  \n",
    "[Loss vs accuracy, differences](https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy)  \n",
    "[Optimal batch size](https://stackoverflow.com/questions/46654424/how-to-calculate-optimal-batch-size)  \n",
    "[Cross_entropy, which one to use](https://stats.stackexchange.com/questions/260505/should-i-use-a-categorical-cross-entropy-or-binary-cross-entropy-loss-for-binary)  \n",
    "\n",
    "**Errors**  \n",
    "[CNN: Errors v1](https://stackoverflow.com/questions/43674411/training-and-loss-not-changing-in-keras-cnn-model)  \n",
    "[CNN: Errors v2](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)  \n",
    "[Training - evaluate loss different v1](https://github.com/keras-team/keras/issues/6977)  \n",
    "[Training - evaluate loss different v1](https://stackoverflow.com/questions/58108543/training-accuracy-while-fitting-the-model-not-reflected-in-confusion-matrix)  \n",
    "[Loss doesn´t decrease v1](https://stackoverflow.com/questions/58237726/keras-model-fails-to-decrease-loss)  \n",
    "[Loss doesn´t decrease v2](https://stackoverflow.com/questions/45577747/cnn-in-tensorflow-loss-remains-constant)  \n",
    "\n",
    "\n",
    "**Examples**  \n",
    "[Alzheimer Diagnosis Git v1](https://github.com/oscardp96/TFM-Alzheimer-Diagnosis)  \n",
    "[Alzheimer Diagnosis Git v2](https://github.com/Ajax121/Alzheimer-s-Classification-and-visualization-using-Convolutional-Neural-networks)  \n",
    "[Animals classification v1](https://www.analyticsvidhya.com/blog/2020/02/learn-image-classification-cnn-convolutional-neural-networks-3-datasets/)  \n",
    "[Animals classification v2](https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/)  \n",
    "[Estimate brain age](https://medium.com/thelaunchpad/how-to-estimate-the-age-of-your-brain-with-mri-data-c60df60da95d)  \n",
    "[Alzheimer Diagnosis 2D Paper v1](https://www.hindawi.com/journals/abb/2021/6690539/)  \n",
    "[Alzheimer Diagnosis Paper v2](https://www.nature.com/articles/s41598-019-54548-6.pdf)  \n",
    "\n",
    "**MRI**  \n",
    "[Alzheimer and hippocampus](https://radiopaedia.org/articles/hippocampus)  \n",
    "[Brain affected by Alzheimer](https://www.nia.nih.gov/health/what-happens-brain-alzheimers-disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys1quYVrXIXQ"
   },
   "source": [
    "### Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622464160519,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "w7cgAHv2XIXd"
   },
   "outputs": [],
   "source": [
    "# Specify if user is working on Google Drive\n",
    "google_drive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22800,
     "status": "ok",
     "timestamp": 1622464185647,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "OR4Iy305XIXe",
    "outputId": "01da16ff-0b87-4ceb-d50f-e3c3a7cc0170"
   },
   "outputs": [],
   "source": [
    "if google_drive == True:\n",
    "    \n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    path = \"./drive/MyDrive/TFM/Code/\"\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append(path)\n",
    "\n",
    "else:\n",
    "    path = \"../\"\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRGI3GB72ifc"
   },
   "source": [
    "#### Google Colab TPU session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1622464192002,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "pljIfdII2doC"
   },
   "outputs": [],
   "source": [
    "# Specify if user is working on a TPU session in Google Colab\n",
    "tpu_session = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13726,
     "status": "ok",
     "timestamp": 1622464206162,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "qIaHwN_O2d2D",
    "outputId": "53acf0bf-e9fc-4f7b-f519-05f590ec1aec"
   },
   "outputs": [],
   "source": [
    "if tpu_session == True:\n",
    "    \n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "    print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "    except ValueError:\n",
    "        raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LYcLeXnjl5q"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1622468381412,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "mQB5P7qbjl5t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "# Import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model \n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, BatchNormalization, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv3D, MaxPool3D, GlobalAveragePooling3D\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, GlobalAveragePooling2D, LSTM\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "#import tensorflow.keras.activations as Activations\n",
    "#import tensorflow.keras.optimizers as Optimizer\n",
    "#import tensorflow.keras.metrics as Metrics\n",
    "#import tensorflow.keras.utils as Utils\n",
    "#from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "# Import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import visualization packages\n",
    "from matplotlib import image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_theme(context='notebook')\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Extra utils\n",
    "from Notebooks.aux_functions.aux_functions_cnn import *\n",
    "\n",
    "#from sklearn.utils import shuffle\n",
    "#from random import randint\n",
    "#from IPython.display import SVG\n",
    "#import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mg0hAAQIjl5v"
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpaSUgs9ZXpY"
   },
   "source": [
    "#### Load CSV files with image details: images IDs and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1622464225897,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "lQvMcnjiZXpa",
    "outputId": "fd67965e-e9c9-4e44-ff1e-33db8449d053"
   },
   "outputs": [],
   "source": [
    "# Load individuals CSV files with image details\n",
    "df_1 = pd.read_csv(path + \"Datasets/ADNI1_Complete_1Yr_1.5T.csv\")\n",
    "df_2 = pd.read_csv(path + \"Datasets/ADNI1_Complete_2Yr_1.5T.csv\")\n",
    "df_3 = pd.read_csv(path + \"Datasets/ADNI1_Complete_3Yr_1.5T.csv\")\n",
    "\n",
    "# Concatenate all CSV files in a unique dataframe\n",
    "df = pd.concat([df_1, df_2, df_3])\n",
    "\n",
    "# Remove extra whitespaces from column names\n",
    "df.columns = df.columns.str.replace(\" \", \"\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1622464231293,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "0b28PAoNZXpc",
    "outputId": "20b789df-df20-4830-cee4-098190b48ac7"
   },
   "outputs": [],
   "source": [
    "# Retrieve only the image ID and Group (class) columns\n",
    "df = df[[\"ImageDataID\", \"Group\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1622464235018,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "ZsW1RHH5ZXpe",
    "outputId": "9a6e9b3d-bd51-4203-a410-45e8299bc33a"
   },
   "outputs": [],
   "source": [
    "# Check number of cases by class in the dataframe\n",
    "df_1[\"Group\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJ5ol7mGZXpe"
   },
   "source": [
    "It can be seen that the three following classes are presented in the dataset:\n",
    "   - **MCI** - Mild cognitive impairment patients\n",
    "   - **CN** - Cognitively normal patients\n",
    "   - **AD** - Alzheimer’s disease patients\n",
    "   \n",
    "In our case, first we will work with only CN and AD cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1622464237827,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "rMtFUQW9ZXpf"
   },
   "outputs": [],
   "source": [
    "# Assign list of images IDs to its corresponding class list: class 0 for CN, class 1 for AD\n",
    "list_class_0 = list(df[df[\"Group\"] == \"CN\"][\"ImageDataID\"])\n",
    "list_class_1 = list(df[df[\"Group\"] == \"AD\"][\"ImageDataID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORYKJ5xtUlt-"
   },
   "source": [
    "#### Load 2D images\n",
    "**NOTE: It takes around 18 minutes to load all 2D images (3287) using TPU session in Google Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1622464239962,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "8arAAAPEZXpk"
   },
   "outputs": [],
   "source": [
    "# Specify folders where there are the 2D images of the brain in PNG format\n",
    "root_png_images = path + \"Datasets/New_png_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1622464241057,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "OBFT77FPZXpl"
   },
   "outputs": [],
   "source": [
    "# Define image coordinates\n",
    "coordinates = [[0,   210, 0,   168], \n",
    "               [0,   210, 168, 336], \n",
    "               [0,   210, 336, 504], \n",
    "               [210, 420, 0,   168], \n",
    "               [210, 420, 168, 336], \n",
    "               [210, 420, 336, 504]]\n",
    "\n",
    "# Initiliaze lists\n",
    "images = []  # List where to save the images\n",
    "titles = []  # List where to save the name of the images (Image ID)\n",
    "\n",
    "# Initialize counters for each class\n",
    "counter_samples_0 = 0\n",
    "counter_samples_1 = 0\n",
    "number_samples = 550  # Specify number of samples desired by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222349,
     "status": "ok",
     "timestamp": 1622464466018,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "FJYjhAGljl5x",
    "outputId": "b441575d-a395-4fbe-e66b-2991ec24923b"
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "for file in os.listdir(root_png_images):\n",
    "  \n",
    "    # Avoid trigerring .DS_Store (when use macOS)\n",
    "    if file.startswith('.DS_Store'):\n",
    "        continue\n",
    "        \n",
    "    # Get image ID of the image\n",
    "    title = file.split(\".\")[0]\n",
    "    \n",
    "    if (title in list_class_0) & (counter_samples_0 < number_samples):\n",
    "        counter_samples_0 += 1\n",
    "    elif (title in list_class_1) & (counter_samples_1 < number_samples):\n",
    "        counter_samples_1 += 1\n",
    "    else:\n",
    "        continue\n",
    "             \n",
    "    # Read image in grayscale\n",
    "    img = cv2.imread(os.path.join(root_png_images,file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Resize image\n",
    "    dim = (504, 420)\n",
    "    resized_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # Split image into 6 slices and place them one after another\n",
    "    divided_img = np.zeros((6, 210, 168))\n",
    "\n",
    "    for index, coordinate in enumerate(coordinates):\n",
    "\n",
    "        coor_y_0, coor_y_1 = coordinates[index][0], coordinates[index][1]\n",
    "        coor_x_0, coor_x_1 = coordinates[index][2], coordinates[index][3] \n",
    "\n",
    "        divided_img[index, :,:] = resized_img[coor_y_0:coor_y_1, coor_x_0:coor_x_1]\n",
    "    \n",
    "    # Append image & image ID to their corresponding lists  \n",
    "    images.append(divided_img)\n",
    "    titles.append(title)\n",
    "    \n",
    "    if len(images) % 200 == 0:\n",
    "        print(f\"{len(images)} images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1622464508722,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "12RzyLPiZXpo",
    "outputId": "879f1af4-5437-4974-ad18-fca83f3ab1c1"
   },
   "outputs": [],
   "source": [
    "# Check number of images loaded\n",
    "print(\"[+] Number of images loaded:\", len(images))\n",
    "print(\"[+] Number of titles loaded:\", len(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61DoGy5nZXpp"
   },
   "source": [
    "#### Visualize loaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9I2mYTszcA2"
   },
   "outputs": [],
   "source": [
    "# Show initial image before being resized\n",
    "image_ = img\n",
    "print(\"[+] Shape of the image:\", image_.shape)\n",
    "plt.figure(figsize = (5, 4))\n",
    "plt.imshow(image_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTsJPr41zcA6"
   },
   "outputs": [],
   "source": [
    "# Show image after being resized\n",
    "image_ = resized_img\n",
    "print(\"[+] Shape of the grayscale image:\", image_.shape)\n",
    "plt.figure(figsize = (5, 4))\n",
    "plt.imshow(image_, cmap = plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 1266,
     "status": "ok",
     "timestamp": 1622481902454,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "vavtrDbTzcA7",
    "outputId": "c1ab3a8f-7fbb-49d6-d765-15d92509bbd3"
   },
   "outputs": [],
   "source": [
    "# Show group of images for a random sample after being splitted into 6 slices\n",
    "image_ = X_normalized[0]\n",
    "figure, axes = plt.subplots(2, 3, figsize = (8, 6))\n",
    "n_plot = 0\n",
    "for i_ in range(2):\n",
    "    for j_ in range(3):\n",
    "    \n",
    "        axes[i_][j_].imshow(image_[n_plot,:,:], cmap = plt.get_cmap('gray'))\n",
    "        axes[i_][j_].axis('on')\n",
    "        n_plot += 1 \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLKW5wLzUluC"
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1622464514314,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "mI6UAGTCjl50",
    "outputId": "222b853b-a705-4248-875b-9d4c8592b6ca"
   },
   "outputs": [],
   "source": [
    "# Retrieve class for each image mapping the titles list and the Image ID column from the dataframe\n",
    "classes = []  # List where to save the class for each image: 1 (AD, MCI), 0 (CN)\n",
    "\n",
    "for title in titles:\n",
    "    \n",
    "    class_ = df[\"Group\"].loc[df['ImageDataID'] == title].values[0]\n",
    "    \n",
    "    if class_ in [\"CN\"]:\n",
    "        classes.append(0)\n",
    "        \n",
    "    elif class_ in [\"AD\"]:\n",
    "        classes.append(1)\n",
    "        \n",
    "    else:\n",
    "        print(f\"ERROR with image ID {title}\")\n",
    "\n",
    "# Check number of cases by class with images loaded\n",
    "print(f\"[+] Number of healthy cases (class 0): {classes.count(0)}\")\n",
    "print(f\"[+] Number of Alzheimer cases (class 1): {classes.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1622481312350,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "xdWScdk5jl52"
   },
   "outputs": [],
   "source": [
    "# Convert X to array\n",
    "X = images\n",
    "X = np.asarray(X).astype('float32')\n",
    "\n",
    "# Convert y to array but one-hot encoding\n",
    "y = np.array(classes).astype('uint8')\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1622464541122,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "9-FYPmQNUluH",
    "outputId": "3dc341cc-15d1-440e-9f10-2f0baa8278b5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check minimum and maximum value of X data\n",
    "print(f\"[+] Minimum value of X data: {np.amin(X)}\")\n",
    "print(f\"[+] Maximum value of X data: {np.amax(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY1bp4RnUluI"
   },
   "source": [
    "It can be seen how images pixel values range from 0 to +255. Typically zero is taken to be black, and 255 is taken to be white. In order to facilitate training, a normalization of the pixel intensity is carried out to have values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1205727,
     "status": "ok",
     "timestamp": 1622465770397,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "ye-BZ8noZXpv",
    "outputId": "8706118b-4371-43fe-9a39-523e3cb3e534",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalizing X data\n",
    "\n",
    "new_min = 0  # Specify new minimum value\n",
    "new_max = 1  # Specify new maximum value\n",
    "\n",
    "X_normalized  = copy.copy(X)\n",
    "\n",
    "for number_sample, sample in enumerate(X_normalized):\n",
    "    \n",
    "    for number_image, image in enumerate(sample): \n",
    "        \n",
    "        I_min = np.amin(image)\n",
    "        I_max = np.amax(image)\n",
    "        \n",
    "        for index_x, item_x in enumerate(image):\n",
    "            for index_y, item_y in enumerate(item_x):\n",
    "\n",
    "                # Exract pixel intensity\n",
    "                I = X_normalized[number_sample][number_image, index_x, index_y]\n",
    "                \n",
    "                # Calculate normalized pizel intensity\n",
    "                I_new = (I - I_min) * (new_max - new_min)/(I_max - I_min)  + new_min\n",
    "                \n",
    "                # Add normalized pixel intensity to array\n",
    "                X_normalized[number_sample][number_image, index_x, index_y] = I_new\n",
    "                \n",
    "    if number_sample % 100 == 0:\n",
    "        print(f\"{number_sample} samples normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1622465777506,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "B6fKpol5ZXp2",
    "outputId": "44040c80-a8a7-44fc-ed53-1b68ad59f50e"
   },
   "outputs": [],
   "source": [
    "print(f\"[+] Minimum value of X data after normalization: {np.amin(X_normalized)}\")\n",
    "print(f\"[+] Maximum value of X data after normalization: {np.amax(X_normalized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxXzPxvEjl53"
   },
   "source": [
    "#### Save images and classes\n",
    "Only run the next two cells if you want to save both X and y arrays into numpy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1622483165241,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "_HhXA0xBiDry",
    "outputId": "fa0a3637-88ed-473b-bcc8-6e04533b7ce6"
   },
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1622483196115,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "klrhedj8jl54"
   },
   "outputs": [],
   "source": [
    "# Save list of images as a numpy file\n",
    "np.savez_compressed(path + \"Datasets/\" + \"titles_list\", titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgvGzlKDUluK"
   },
   "outputs": [],
   "source": [
    "# Save list of classes as a numpy file\n",
    "np.savez_compressed(\".\" + path + \"Datasets/\" + \"classes\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3uDh9j9jl55"
   },
   "source": [
    "#### Load images and classes\n",
    "Only run the next two cells if you want to load both X and y arrays from numpy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heeCArQWUluL"
   },
   "outputs": [],
   "source": [
    "# Load images in array format\n",
    "loaded_images = np.load('images.npz', allow_pickle= True)\n",
    "X = loaded_images['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA9E66c6jl56"
   },
   "outputs": [],
   "source": [
    "# Load classes in array format\n",
    "loaded_classes = np.load('classes.npz', allow_pickle= True)\n",
    "y = loaded_classes['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-OGSZ9vjl56"
   },
   "source": [
    "#### Get training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 971,
     "status": "ok",
     "timestamp": 1622481830028,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "HA4wEN72jl57",
    "outputId": "c88b82aa-149c-42f6-a665-b11f8e109fbc"
   },
   "outputs": [],
   "source": [
    "# Define testing proportion of the dataset\n",
    "test_size = 0.2\n",
    "validation_size = 0.2\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, \n",
    "                                                    test_size = test_size, \n",
    "                                                    random_state = 7)\n",
    "\n",
    "# Extract validation data from training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size = validation_size, \n",
    "                                                  random_state = 7)\n",
    "\n",
    "# Print data shapes\n",
    "print(f\"[+] Training shape: {len(X_train)}\")\n",
    "print(f\"    [-] Number of healthy cases: {list(map(sum, zip(*y_train)))[0]}\")\n",
    "print(f\"    [-] Number of Alzheimer cases: {list(map(sum, zip(*y_train)))[1]}\")\n",
    "\n",
    "print(f\"\\n[+] Validation shape: {len(X_val)}\")\n",
    "print(f\"    [-] Number of healthy cases: {list(map(sum, zip(*y_val)))[0]}\")\n",
    "print(f\"    [-] Number of Alzheimer cases: {list(map(sum, zip(*y_val)))[1]}\")\n",
    "\n",
    "print(f\"\\n[+] Testing shape: {len(X_test)}\")\n",
    "print(f\"    [-] Number of healthy cases: {list(map(sum, zip(*y_test)))[0]}\")\n",
    "print(f\"    [-] Number of Alzheimer cases: {list(map(sum, zip(*y_test)))[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PihvLS1Gjl58"
   },
   "source": [
    "### Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_3d(model_name, input_shape):\n",
    "    '''\n",
    "    Build a 3D Convolutional Neural Network (CNN).\n",
    "    '''\n",
    "\n",
    "    # Fix random seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "    tf.random.set_seed(123) \n",
    "    \n",
    "    ## Input layer\n",
    "    inputs = Input(shape = input_shape + (1,))\n",
    "    \n",
    "    ## Convolutional blocks\n",
    "    # 1st conv block\n",
    "    x = Conv3D(64, kernel_size = 3, activation = 'relu')(inputs)\n",
    "    x = MaxPool3D(pool_size = 2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "              \n",
    "    # 2nd conv block\n",
    "    x = Conv3D(128, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = MaxPool3D(pool_size = 2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 3rd conv block\n",
    "    x = Conv3D(256, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = MaxPool3D(pool_size = 2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 4rd conv block\n",
    "    x = Conv3D(512, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = MaxPool3D(pool_size = 2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    ## Flatten layer\n",
    "    x = GlobalAveragePooling3D()(x)\n",
    "              \n",
    "    ## Dense layers       \n",
    "    x = Dense(512, activation = \"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(256, activation = \"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    ## Output layer\n",
    "    outputs = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Name model\n",
    "    model._name = model_name\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPwiNPnOkzZt"
   },
   "source": [
    "#### Build new CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1622482008517,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "lNMSRcnFkzZt"
   },
   "outputs": [],
   "source": [
    "# Define inputs\n",
    "model_name = \"3d_model_v1\"\n",
    "input_shape = (128, 128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = build_model_3d(model_name, input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgJFl2xUkzZu"
   },
   "source": [
    "#### Load existing CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gHzzLDMkzZv"
   },
   "outputs": [],
   "source": [
    "model_name = \"cnn_model_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuPkzTQTkzZw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_loaded = load_model(path + \"Results/\" + model_name + \".h5\", \n",
    "                   custom_objects = {'f1': f1})\n",
    "\n",
    "print(\"[+] Model loaded\")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kGYgaDGkzZw"
   },
   "outputs": [],
   "source": [
    "# Load model history\n",
    "history_loaded = np.load(path + \"Results/\" + model_name + \"_history.npy\", \n",
    "                  allow_pickle = 'TRUE').item()\n",
    "\n",
    "print(\"[+] Model history loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5o7hki78UluS"
   },
   "source": [
    "### Train CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1622465824400,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "fq3UGsn_UluT"
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1622482155719,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "4fHz567hfJYT"
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = SGD(learning_rate = 0.1)  # from 0.0001 to 0.1\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"BinaryAccuracy\", f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 432798,
     "status": "ok",
     "timestamp": 1622482637902,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "gq7rPP8wjl5-",
    "outputId": "c6c34812-c668-4697-849e-4274de921c02",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    " \n",
    "history = model.fit(x = X_train, \n",
    "                    y = y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 32,\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    verbose = 1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\n[+] Time of training: \"+\"{:.2f}\".format(end_time-start_time));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eR9YKWIj5CVt"
   },
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1622482642893,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "o9ewdnA_jl5_",
    "outputId": "743201f3-dc23-42ae-d53c-1c41b5bb85ce"
   },
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13236,
     "status": "ok",
     "timestamp": 1622482860663,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "1KL2_D0XzcBr",
    "outputId": "55069a31-8b42-4a6b-b5fa-ef5b255325f6"
   },
   "outputs": [],
   "source": [
    "# Get accuracy\n",
    "model.evaluate(x = X_train,\n",
    "               y = y_train, \n",
    "               verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1622482742932,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "Y5SJZE4oHhJo",
    "outputId": "9541c14d-f963-4b43-eb8c-985c969720a4"
   },
   "outputs": [],
   "source": [
    "model.predict(X_train[0].reshape(1,6,210,168))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1622479347430,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "mYO8qnapPTJz"
   },
   "outputs": [],
   "source": [
    "new_x = np.reshape(x, (1,6,210,168))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1622478892846,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "AqpAlOI0PbuF",
    "outputId": "e3732480-15e9-402f-d065-b813ece6b844"
   },
   "outputs": [],
   "source": [
    "model.predict(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "123kCE95kzZz"
   },
   "outputs": [],
   "source": [
    "# Get predictions TEST\n",
    "y_predict = get_predictions(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfGDt_RdkzZ0"
   },
   "outputs": [],
   "source": [
    "# Get evaluation TEST\n",
    "get_evaluation(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OjuASrTzcBw"
   },
   "outputs": [],
   "source": [
    "############# TRAIN EVALUATION ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12522,
     "status": "ok",
     "timestamp": 1622476071592,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "DFtnh6K0zcBw"
   },
   "outputs": [],
   "source": [
    "# Get predictions TRAIN\n",
    "y_predict = get_predictions(X_train, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1622476077682,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "t8mrUcZIzcBw",
    "outputId": "6270912f-91b1-41d2-c13b-ed7a924c8a7a"
   },
   "outputs": [],
   "source": [
    "# Get evaluation TRAIN\n",
    "get_evaluation(y_train, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWCgkoSakzZ0"
   },
   "source": [
    "- **Label 0** = healthy cases\n",
    "- **Label 1** = Alzheimer or MCI cases  \n",
    " \n",
    "**Correct:** 74 cases were correctly classified as healty, whilst 42 cases were correctly classified as Alzheimer.  \n",
    "**Incorrect:** 13 cases that were healty were classified as Alzheimer.  \n",
    "**URGENT:** 21 cases that had Alzheimer were classified as healty. !!!\n",
    "\n",
    "The main goal is to reduce the number of False Negatives (FN), it means the number of cases with Alzheimer that has been classifed as healty. To achieve that, Recall must be as close as possible to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpqWaJEAkzZ0"
   },
   "source": [
    "<div>\n",
    "<img src=\"./pictures/cm_description.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLkit37YUluX"
   },
   "outputs": [],
   "source": [
    "# Plot roc curve\n",
    "plot_roc_curve(model, X_train, X_test, y_train, y_test, save_fig = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB3fGX0xUluZ"
   },
   "source": [
    "#### Save CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1329,
     "status": "ok",
     "timestamp": 1622478921138,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "L2JrB7wmrRdb",
    "outputId": "08b099c3-c42e-4cf8-9e72-32977edc0e48"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(path + \"Results/\" + model.name + \".h5\")\n",
    "\n",
    "print(\"[+] Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1622478922683,
     "user": {
      "displayName": "Fernando Herran",
      "photoUrl": "",
      "userId": "09119314463851443877"
     },
     "user_tz": -120
    },
    "id": "6WeW0OBg5sdw",
    "outputId": "4839836c-da57-46ce-bdc8-7067ca9f673d"
   },
   "outputs": [],
   "source": [
    "# Save model history\n",
    "np.save(path + \"Results/\" + model.name + \"_history.npy\", history.history)\n",
    "\n",
    "print(\"[+] Model history saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRLNIT0SUlua"
   },
   "source": [
    "===================================================================================================================\n",
    "#==================================================================================================================\n",
    "### OTHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YEnJ-QZUlua"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.inception_v3.InceptionV3(\n",
    "    input_shape=(240, 320, 3), \n",
    "    weights='imagenet', \n",
    "    include_top=False,\n",
    "    pooling='max')\n",
    "\n",
    "base_output = base_model.output\n",
    "hidden_layer = tf.keras.layers.Dense(512, activation='relu')(base_output)\n",
    "hl_reg = tf.keras.layers.Dropout(0.8)(hidden_layer)\n",
    "output_layer = tf.keras.layers.Dense(1, activation='softmax')(hl_reg)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UswN-Fd9Ulua"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6ZsZLedUlub"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "dropout = 0.1\n",
    "mn_value = 0.5\n",
    "\n",
    "# Start model\n",
    "m = Models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "m.add(Layers.Flatten(input_shape=(240, 320, 3)))\n",
    "\n",
    "# Hidden layers\n",
    "m.add(Layers.Dense(units=10, activation = \"relu\", kernel_constraint=max_norm(mn_value)))\n",
    "m.add(Layers.Dense(units=10, activation = \"relu\", kernel_constraint=max_norm(mn_value)))\n",
    "m.add(Layers.Dense(units=10, activation = \"relu\", kernel_constraint=max_norm(mn_value)))\n",
    "m.add(Layers.Dropout(rate = dropout))\n",
    "\n",
    "# Output layer\n",
    "m.add(Layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "m.compile(optimizer = \"adam\" ,\n",
    "          loss = \"binary_crossentropy\",\n",
    "          metrics = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PD6QMt0XzcB4"
   },
   "outputs": [],
   "source": [
    "# Define Hyperparameters  \n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "steps_per_epoch = int(len(X_train) * 0.85/ batch_size)\n",
    "validation_split = 0.3\n",
    "validation_steps = int(len(X_train) * validation_split / batch_size)\n",
    "\n",
    "print(\"[+] Steps per epoch:\", steps_per_epoch)\n",
    "print(\"[+] Validation steps:\", validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhlSQaxAzcB5"
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, \n",
    "                                                          decay_steps = 100000, \n",
    "                                                          decay_rate = 0.96, \n",
    "                                                          staircase = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-m3UWK_zcB5"
   },
   "outputs": [],
   "source": [
    "def get_model(model_name, input_shape_):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input(input_shape_)\n",
    "\n",
    "    x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool2D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrfNEHM-zcB6"
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((210, 168, 6, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=(3,3,1), activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=(2,2,1))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=(3,3,1), activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=(2,2,1))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=(3,3,1), activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=(2,2,1))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=(3,3,1), activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=(2,2,1))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vdpd8FD1zcB8"
   },
   "outputs": [],
   "source": [
    "def build_model(model_name, input_shape_):\n",
    "    '''\n",
    "    Function to build a convolutional neural network.\n",
    "    Inputs: input shape\n",
    "    '''\n",
    "\n",
    "    # Fix random seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "    tf.random.set_seed(123) \n",
    "\n",
    "    # Start Sequential model\n",
    "    #model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape = (210, 168, 1), batch_size=6 )\n",
    "              \n",
    "    # Convolutional layers  \n",
    "    ############################################\n",
    "    \n",
    "    ## 1st conv block\n",
    "    \n",
    "    x = Conv2D(filters = 105, kernel_size = 3, activation = 'relu')(inputs)\n",
    "    x = MaxPool2D(pool_size = 2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "              \n",
    "    ############################################\n",
    "              \n",
    "    ## 2nd conv block\n",
    "    x = Conv2D(filters = 75, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = MaxPool2D(pool_size = 2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "              \n",
    "    ############################################\n",
    "              \n",
    "    ## 3rd conv block\n",
    "    x = Conv2D(filters = 125, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = MaxPool2D(pool_size = 2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Layer yo convert a 3D tensor to 1D tensor\n",
    "    #x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "              \n",
    "    # Hidden layers       \n",
    "    x = Dense(150, activation = \"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    #x = Dense(100, activation = \"relu\")(x)\n",
    "    #x = Dropout(0.3)(x)\n",
    "    \n",
    "    #x = Dense(50, activation = \"relu\")(x)\n",
    "    #x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Name model\n",
    "    model._name = model_name\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8y7xS87jZXqY"
   },
   "outputs": [],
   "source": [
    "# Capture 1000 samples for each class\n",
    "number_healty = 0\n",
    "number_disease = 0\n",
    "images_2 = []\n",
    "classes_2 = []\n",
    "n_samples = 500\n",
    "\n",
    "for index, item_ in enumerate(classes):\n",
    "    \n",
    "    if (item_ == 1) & (number_disease < n_samples):\n",
    "        classes_2.append(1)\n",
    "        images_2.append(images[index])\n",
    "        number_disease += 1\n",
    "  \n",
    "    if (item_ == 0) & (number_healty < n_samples):\n",
    "        classes_2.append(0)\n",
    "        images_2.append(images[index])\n",
    "        number_healty += 1\n",
    "\n",
    "# Check number of cases by class with images loaded\n",
    "print(f\"[+] Number of total cases: {len(images_2)}\")\n",
    "print(f\"    [-] Number of Alzheimer cases: {classes_2.count(1)}\")\n",
    "print(f\"    [-] Number of healthy cases: {classes_2.count(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLxB_x_RZXqY"
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "for file in os.listdir(root_png_images):\n",
    "  \n",
    "    # Avoid trigerring .DS_Store (when use macOS)\n",
    "    if file.startswith('.DS_Store'):\n",
    "        continue\n",
    "        \n",
    "    # Get title of the image\n",
    "    title = file.split(\".\")[0]\n",
    "    \n",
    "    if (title in list_class_0) & (number_samples_0 < 500):\n",
    "        number_samples_0 += 1\n",
    "    elif (title in list_class_1) & (number_samples_1 < 500):\n",
    "        number_samples_1 += 1\n",
    "    else:\n",
    "        continue\n",
    "             \n",
    "    # Read image in grayscale\n",
    "    img = cv2.imread(os.path.join(root_png_images,file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Resize image\n",
    "    # https://www.tutorialkart.com/opencv/python/opencv-python-resize-image/\n",
    "    # https://enmilocalfunciona.io/tratamiento-de-imagenes-usando-imagedatagenerator-en-keras/\n",
    "    dim = (504, 420)\n",
    "    resized_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    # Convert image from RGBA2RGB\n",
    "    #if len(resized_img.shape) > 2 and resized_img.shape[2] == 4:\n",
    "    #    resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGRA2BGR)\n",
    "        \n",
    "    # Convert image from RGB to grayscale\n",
    "    #gray_img = rgb2gray(resized_img) \n",
    "    \n",
    "    # Split image into 6 slices\n",
    "    divided_img = np.zeros((6, 210, 168))\n",
    "\n",
    "    for index, coordinate in enumerate(coordinates):\n",
    "\n",
    "        coor_y_0, coor_y_1 = coordinates[index][0], coordinates[index][1]\n",
    "        coor_x_0, coor_x_1 = coordinates[index][2], coordinates[index][3] \n",
    "\n",
    "        divided_img[index, :,:] = resized_img[coor_y_0:coor_y_1, coor_x_0:coor_x_1]\n",
    "    \n",
    "    # Append image & title to lists  \n",
    "    images.append(divided_img)\n",
    "    titles.append(title)\n",
    "    \n",
    "    if len(images) % 200 == 0:\n",
    "        print(f\"{len(images)} images loaded\")\n",
    "    \n",
    "# Check number of images loaded\n",
    "print(\"[+] Number of images loaded:\", len(images))\n",
    "print(\"[+] Number of titles loaded:\", len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name, input_shape, number_samples):\n",
    "    '''\n",
    "    Function to build a convolutional neural network.\n",
    "    Inputs: input shape\n",
    "    '''\n",
    "\n",
    "    # Fix random seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "    tf.random.set_seed(123) \n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape = number_samples + input_shape)\n",
    "              \n",
    "    ## 1st conv block\n",
    "    x = TimeDistributed(Conv2D(64, kernel_size = 3, activation = 'relu'))(inputs)\n",
    "    x = TimeDistributed(MaxPool2D(pool_size = 2))(x)\n",
    "    x = TimeDistributed(BatchNormalization())(x)\n",
    "              \n",
    "    ## 2nd conv block\n",
    "    x = TimeDistributed(Conv2D(64, kernel_size = 3, activation = 'relu'))(x)\n",
    "    x = TimeDistributed(MaxPool2D(pool_size = 2))(x)\n",
    "    x = TimeDistributed(BatchNormalization())(x)\n",
    "\n",
    "    ## 3rd conv block\n",
    "    x = TimeDistributed(Conv2D(128, kernel_size = 3, activation = 'relu'))(x)\n",
    "    x = TimeDistributed(MaxPool2D(pool_size = 2))(x)\n",
    "    x = TimeDistributed(BatchNormalization())(x)\n",
    "\n",
    "    ## 4rd conv block\n",
    "    x = TimeDistributed(Conv2D(256, kernel_size = 3, activation = 'relu'))(x)\n",
    "    x = TimeDistributed(MaxPool2D(pool_size = 2))(x)\n",
    "    x = TimeDistributed(BatchNormalization())(x)\n",
    "    \n",
    "    ## Flatten layer\n",
    "    x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
    "    \n",
    "    # LSTM\n",
    "    x = LSTM(1024, activation = 'relu', return_sequences = False)(x)\n",
    "              \n",
    "    # Hidden layers       \n",
    "    x = Dense(1024, activation = \"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation = \"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(2, activation = \"softmax\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Name model\n",
    "    model._name = model_name\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_CNN_creation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
