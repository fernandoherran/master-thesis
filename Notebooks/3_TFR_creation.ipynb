{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"tfm_env","language":"python","name":"tfm_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"3_TFR_creation.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"rVsfg8weBKcx"},"source":["## TFRecords creation\n","The goal of this Notebook is to **convert all the images that have been preprocessed in the Notebook 2_MRI_preprocessing and saved as a numpy’s compressed format (.npz) in the folder 'Datasets/Image_files' to TFRecords.**\n","\n","According to [Tensorflow](https://www.tensorflow.org/tutorials/load_data/tfrecord), TFRecords are used to store the data as a sequence of binary strings. The main advantage of using TFRecords is that it speeds up data reading.\n","\n","This notebook is structured as follows:\n","   - Initial set-up\n","   - Import libraries\n","   - Load features\n","   - Load labels\n","   - Create TFRecords"]},{"cell_type":"markdown","metadata":{"id":"Nu6CFWX7BKc1"},"source":["### Initial set-up"]},{"cell_type":"markdown","metadata":{"id":"ErrixVFNBKc2"},"source":["#### Google Colab"]},{"cell_type":"code","metadata":{"id":"v7ywVwLNBKc3","executionInfo":{"status":"ok","timestamp":1626803093217,"user_tz":-120,"elapsed":13,"user":{"displayName":"Fernando Herran","photoUrl":"","userId":"09119314463851443877"}}},"source":["# Specify if user is working on Google Drive\n","google_colab = True"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R87yUdRcBKc4","executionInfo":{"status":"ok","timestamp":1626803167832,"user_tz":-120,"elapsed":43050,"user":{"displayName":"Fernando Herran","photoUrl":"","userId":"09119314463851443877"}},"outputId":"45b1109d-5b7c-45c5-9115-310c32dde9a6"},"source":["if google_colab == True:\n","    \n","    from google.colab import drive \n","    drive.mount('/content/drive')\n","    \n","    path = './drive/MyDrive/TFM/Code/'\n","    \n","    import os\n","    os.chdir(path)\n","\n","else:\n","    path = '../'\n","    \n","    import os\n","    os.chdir(path)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F61-09-wBKc5"},"source":["### Import libraries"]},{"cell_type":"code","metadata":{"id":"krF9gmZWBKc6","executionInfo":{"status":"ok","timestamp":1626803178408,"user_tz":-120,"elapsed":4486,"user":{"displayName":"Fernando Herran","photoUrl":"","userId":"09119314463851443877"}}},"source":["import os\n","import random\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","import tensorflow as tf\n","import glob"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WcYUQqsmBKc7"},"source":["### Load features"]},{"cell_type":"markdown","metadata":{"id":"s3To2W-WBKc8"},"source":["#### Load 3D images directories (features)"]},{"cell_type":"code","metadata":{"id":"FKdbRlNTBKc8","executionInfo":{"status":"ok","timestamp":1626803230440,"user_tz":-120,"elapsed":505,"user":{"displayName":"Fernando Herran","photoUrl":"","userId":"09119314463851443877"}}},"source":["# Specify folder where there are the 3D images in numpy’s compressed format (.npz)\n","directory_images = './Datasets/Image_files/'"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSetKI2EBKc9","executionInfo":{"status":"ok","timestamp":1626803241062,"user_tz":-120,"elapsed":8888,"user":{"displayName":"Fernando Herran","photoUrl":"","userId":"09119314463851443877"}},"outputId":"17169aac-042d-418b-e4d0-af446b31943d"},"source":["# Get list of filenames from 3D volumes\n","filenames = os.listdir(directory_images)\n","\n","# Remove \".DS_Store\" file from list\n","if '.DS_Store' in filenames:\n","    filenames.remove('.DS_Store')\n","\n","# Include all the path for each file\n","filenames = [directory_images + file for file in filenames]\n","\n","# Check number of images loaded\n","print('[+] Number of 3D images:', len(filenames))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[+] Number of 3D images: 1146\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4lWlKHh4BKc_","executionInfo":{"status":"ok","timestamp":1626803252242,"user_tz":-120,"elapsed":279,"user":{"displayName":"Fernando Herran","photoUrl":"","userId":"09119314463851443877"}}},"source":["# Shuffle list of filenames\n","random.shuffle(filenames)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wM6ummVvBKdA"},"source":["#### Split dataset into training, validation and testing"]},{"cell_type":"code","metadata":{"id":"xiN9jZnIBKdA"},"source":["train_size = 0.7\n","test_size = 0.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ACqUkmmBKdB"},"source":["train_filenames, test_filenames = train_test_split(filenames, \n","                                               test_size = 0.3, \n","                                               random_state = 7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62fW73hCBKdC"},"source":["test_filenames, val_filenames = train_test_split(test_filenames, \n","                                             test_size = 0.5, \n","                                             random_state = 7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NotWZBXmBKdC","outputId":"34cd3536-467e-4fb8-f894-5d19454c17d7"},"source":["# Check size of each dataset\n","print(f'[+] Training size:', len(train_filenames))\n","print(f'[+] Validation size:', len(val_filenames))\n","print(f'[+] Testing size:', len(test_filenames))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[+] Training size: 802\n","[+] Validation size: 172\n","[+] Testing size: 172\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BFwzj7U-BKdD"},"source":["#### Load array for each 3D image (features)"]},{"cell_type":"code","metadata":{"id":"c_33i81QBKdE"},"source":["def return_volumes(filenames):\n","    '''\n","    Function used to load numpy arrays (.npz) from a list of directories\n","    Input: directories where there are the numpy arrays\n","    Output: array with 3D images\n","    ''' \n","\n","    # List where to save the 3D images\n","    volumes = []\n","\n","    # Load 3D images from filenames list\n","    for file in filenames:\n","\n","        # Read 3D image\n","        volume = np.load(file, allow_pickle= True)['arr_0']\n","\n","        # Append 3D image to volumes list \n","        volumes.append(volume)\n","        \n","        if len(volumes) % 100 == 0:\n","            print('[+] Number of images loaded:', len(volumes))\n","    \n","    print('Total number of images loaded:', len(volumes))\n","        \n","    return np.array(volumes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POJWJUn4BKdE","outputId":"c7f0cbaa-0806-40d2-b3e1-2bb3fc76bfac"},"source":["# Load training images\n","train_dataset = return_volumes(train_filenames)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[+] Number of images loaded: 100\n","[+] Number of images loaded: 200\n","[+] Number of images loaded: 300\n","[+] Number of images loaded: 400\n","[+] Number of images loaded: 500\n","[+] Number of images loaded: 600\n","[+] Number of images loaded: 700\n","[+] Number of images loaded: 800\n","Total number of images loaded: 802\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cT8V31ZhBKdF","outputId":"215c374f-65c0-460c-91ef-dc5183230238"},"source":["# Load validation images\n","val_dataset = return_volumes(val_filenames)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[+] Number of images loaded: 100\n","Total number of images loaded: 172\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wj9ItHizBKdG","outputId":"b8344f7c-842d-41d1-b971-6c0f9484e232"},"source":["# Load testing images\n","test_dataset = return_volumes(test_filenames)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[+] Number of images loaded: 100\n","Total number of images loaded: 172\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YO8ihdVlBKdH","outputId":"ad6e3031-1eff-4a7a-ae1a-e95b6a888897"},"source":["# Check size of each dataset\n","print(f'[+] Training shape:', train_dataset.shape)\n","print(f'[+] Validation shape:', val_dataset.shape)\n","print(f'[+] Testing shape:', test_dataset.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[+] Training shape: (802, 110, 130, 80)\n","[+] Validation shape: (172, 110, 130, 80)\n","[+] Testing shape: (172, 110, 130, 80)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gcoky-wzBKdH"},"source":["### Load labels"]},{"cell_type":"markdown","metadata":{"id":"qPHuLNrIBKdI"},"source":["#### Load CSV files with image details: images IDs and class"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"R6DWARq7BKdJ","executionInfo":{"status":"ok","timestamp":1626803284000,"user_tz":-120,"elapsed":957,"user":{"displayName":"Fernando Herran","photoUrl":"","userId":"09119314463851443877"}},"outputId":"92883b0f-5bb7-4239-ee79-6bd017bdd8b3"},"source":["# Load individuals CSV files with image details\n","df_1 = pd.read_csv('./Datasets/ADNI1_Complete_1Yr_1.5T.csv')\n","df_2 = pd.read_csv('./Datasets/ADNI1_Complete_2Yr_1.5T.csv')\n","df_3 = pd.read_csv('./Datasets/ADNI1_Complete_3Yr_1.5T.csv')\n","\n","# Concatenate all CSV files in a unique dataframe\n","df = pd.concat([df_1, df_2, df_3])\n","\n","# Remove extra whitespaces from column names\n","df.columns = df.columns.str.replace(\" \", \"\")\n","\n","df.head()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ImageDataID</th>\n","      <th>Subject</th>\n","      <th>Group</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>Visit</th>\n","      <th>Modality</th>\n","      <th>Description</th>\n","      <th>Type</th>\n","      <th>AcqDate</th>\n","      <th>Format</th>\n","      <th>Downloaded</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I125941</td>\n","      <td>137_S_1426</td>\n","      <td>MCI</td>\n","      <td>M</td>\n","      <td>85</td>\n","      <td>4</td>\n","      <td>MRI</td>\n","      <td>MPR-R; GradWarp; N3; Scaled</td>\n","      <td>Processed</td>\n","      <td>10/30/2008</td>\n","      <td>NiFTI</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I121703</td>\n","      <td>128_S_1408</td>\n","      <td>MCI</td>\n","      <td>M</td>\n","      <td>73</td>\n","      <td>4</td>\n","      <td>MRI</td>\n","      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n","      <td>Processed</td>\n","      <td>9/19/2008</td>\n","      <td>NiFTI</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I121637</td>\n","      <td>037_S_1421</td>\n","      <td>MCI</td>\n","      <td>F</td>\n","      <td>76</td>\n","      <td>4</td>\n","      <td>MRI</td>\n","      <td>MPR; GradWarp; N3; Scaled</td>\n","      <td>Processed</td>\n","      <td>9/17/2008</td>\n","      <td>NiFTI</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I122382</td>\n","      <td>128_S_1407</td>\n","      <td>MCI</td>\n","      <td>F</td>\n","      <td>76</td>\n","      <td>4</td>\n","      <td>MRI</td>\n","      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n","      <td>Processed</td>\n","      <td>9/05/2008</td>\n","      <td>NiFTI</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I121689</td>\n","      <td>127_S_1427</td>\n","      <td>MCI</td>\n","      <td>F</td>\n","      <td>71</td>\n","      <td>4</td>\n","      <td>MRI</td>\n","      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n","      <td>Processed</td>\n","      <td>9/02/2008</td>\n","      <td>NiFTI</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  ImageDataID     Subject Group Sex  ...       Type     AcqDate Format Downloaded\n","0     I125941  137_S_1426   MCI   M  ...  Processed  10/30/2008  NiFTI        NaN\n","1     I121703  128_S_1408   MCI   M  ...  Processed   9/19/2008  NiFTI        NaN\n","2     I121637  037_S_1421   MCI   F  ...  Processed   9/17/2008  NiFTI        NaN\n","3     I122382  128_S_1407   MCI   F  ...  Processed   9/05/2008  NiFTI        NaN\n","4     I121689  127_S_1427   MCI   F  ...  Processed   9/02/2008  NiFTI        NaN\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"XiQ0nPuaBKdK","outputId":"818ff451-bcc9-4b3b-e716-f050bea31f47"},"source":["# Retrieve only the image ID and Group (class) columns\n","df = df[['ImageDataID', 'Group']]\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ImageDataID</th>\n","      <th>Group</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I125941</td>\n","      <td>MCI</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I121703</td>\n","      <td>MCI</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I121637</td>\n","      <td>MCI</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I122382</td>\n","      <td>MCI</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I121689</td>\n","      <td>MCI</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  ImageDataID Group\n","0     I125941   MCI\n","1     I121703   MCI\n","2     I121637   MCI\n","3     I122382   MCI\n","4     I121689   MCI"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"bIHx84pBBKdL"},"source":["#### Load class for each 3D volume (labels)"]},{"cell_type":"code","metadata":{"id":"wHvyS1h4BKdM"},"source":["def return_labels(filenames, df):\n","    '''\n","    Function used to retrieve label for each sample of a dataset\n","    Input: directories where there are the images\n","    Output: array with labels\n","    ''' \n","\n","    # List where to save the class for each image: 0 (CN), 1 (AD)\n","    labels = []\n","\n","    # Load labels from filenames list\n","    for file in filenames:\n","\n","        # Get image ID from file name\n","        image_id = file.split('/')[-1].split('.')[0]\n","\n","        # Retrieve class from dataframe searching by the image ID\n","        label = df['Group'].loc[df['ImageDataID'] == image_id].values[0]\n","\n","        # Assign a class numerical value depending on the group: 0 (CN) or 1 (AD)\n","        if label in ['CN']:\n","            labels.append([0]) \n","        elif label in ['AD']:\n","            labels.append([1])  \n","        else:\n","            print(f'ERROR with image ID {image_id}')\n","    \n","    return np.array(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxwIpSKhBKdN"},"source":["# Load training labels\n","train_labels = return_labels(train_filenames, df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yReDd7rLBKdO"},"source":["# Load validation labels\n","val_labels = return_labels(val_filenames, df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VflOXS8JBKdP"},"source":["# Load testing labels\n","test_labels = return_labels(test_filenames, df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQq7hnJyBKdQ","outputId":"f85443aa-b4de-458d-c8f2-1948c9c012b4"},"source":["# Check size of each dataset\n","print(f'[+] Training labels shape:', train_labels.shape)\n","print(f'[+] Validation labels shape:', val_labels.shape)\n","print(f'[+] Testing labels shape:', test_labels.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[+] Training labels shape: (802, 1)\n","[+] Validation labels shape: (172, 1)\n","[+] Testing labels shape: (172, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L3rJBAogBKdT"},"source":["### Create TFRecords\n","The following functions have been already defined by Tensorflow, and can be found in this [link](https://www.tensorflow.org/tutorials/load_data/tfrecord)."]},{"cell_type":"markdown","metadata":{"id":"7NW5XD_bBKdV"},"source":["#### Define functions"]},{"cell_type":"code","metadata":{"id":"72L3UmBZBKdX"},"source":["def _bytes_feature(value):\n","    '''\n","    Returns a bytes_list from a string / byte.\n","    '''\n","    \n","    if isinstance(value, type(tf.constant(0))): # if value is tensor\n","        value = value.numpy() # get value of tensor\n","    \n","    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","\n","def _float_feature(value):\n","    '''\n","    Returns a floast_list from a float / double.\n","    '''\n","    \n","    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n","\n","def _int64_feature(value):\n","    '''\n","    Returns an int64_list from a bool / enum / int / uint.\n","    '''\n","    \n","    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n","\n","def serialize_array(array):\n","    \n","    array = tf.io.serialize_tensor(array)\n","    \n","    return array\n","\n","def parse_single_volume(volume, label):\n","    \n","    # Get first value of label, as it is an array of length 1\n","    label = label[0]\n","    \n","    # Define the dictionary -- the structure -- of our single example\n","    data = {'height' : _int64_feature(volume.shape[0]),\n","            'width' : _int64_feature(volume.shape[1]),\n","            'depth' : _int64_feature(volume.shape[2]),\n","            'raw_image' : _bytes_feature(serialize_array(volume)),\n","            'label' : _int64_feature(label)}\n","    \n","    # Create an Example, wrapping the single features\n","    out = tf.train.Example(features = tf.train.Features(feature = data))\n","\n","    return out\n","\n","def write_images_to_tfr(volumes, labels, filename = 'images', max_files = 10, out_dir = './Datasets/TFRecords/'):\n","\n","    # Determine the number of TFRecords needed\n","    splits = (len(volumes)//max_files) + 1 \n","    if len(volumes) % max_files == 0:\n","        splits-=1   \n","    \n","    print(f'[+] Number of TFRecords needed for {len(volumes)} volumes: {splits}')\n","    print(f'    [-] Number of files per TFRecord: {max_files}')\n","    \n","    # Check if output directory exists\n","    if not os.path.exists(out_dir):\n","        os.mkdir(out_dir)   \n","    print(f'\\n[+] Output directory: {out_dir}\\n')\n","    \n","    # Write TFRecords\n","    file_count = 0\n","    \n","    for i in tqdm.tqdm(range(splits)):\n","        \n","        # Retrieve name of the TFRecord\n","        tfr_name = '{}{}_{}.tfrecords'.format(out_dir, i+1, filename)\n","        print(f'[+] Writing TFRecord: {tfr_name}')\n","\n","        # Start writer\n","        writer = tf.io.TFRecordWriter(tfr_name)\n","        current_tfr_count = 0\n","    \n","        while current_tfr_count < max_files: \n","            \n","            # Get the index of the file that we want to parse now\n","            index = i * max_files + current_tfr_count\n","            \n","            # Check if all dataset has been added to TFRecords\n","            if index == len(volumes):\n","                break\n","                \n","            # Retrieve volume and label\n","            current_volume = volumes[index]\n","            current_label = labels[index]\n","\n","            # Create the required example representation\n","            out = parse_single_volume(volume = current_volume, label = current_label)\n","\n","            writer.write(out.SerializeToString())\n","            \n","            # Update counters\n","            current_tfr_count+=1\n","            file_count += 1\n","       \n","        # Close writer\n","        writer.close()\n","    \n","    print(f'Number of files wrote to TFRecords: {file_count}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wpV_Che9BKdY"},"source":["#### Create training TFRecords"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"FIZNxmpKBKdZ","outputId":"4ad6f9b1-498e-48f1-821c-2d071a32b08e"},"source":["write_images_to_tfr(train_dataset, train_labels, \n","                    max_files = 30, \n","                    filename = 'train_volumes',\n","                    out_dir = './Datasets/TFRecords/Train/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[+] Number of TFRecords needed for 802 volumes: 27\n","    [-] Number of files per TFRecord: 30\n","\n","[+] Output directory: ../Datasets/TFRecords/Train/\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/27 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/1_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r  4%|▎         | 1/27 [00:01<00:40,  1.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/2_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r  7%|▋         | 2/27 [00:03<00:38,  1.54s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/3_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 11%|█         | 3/27 [00:05<00:43,  1.82s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/4_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 15%|█▍        | 4/27 [00:07<00:42,  1.86s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/5_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 19%|█▊        | 5/27 [00:09<00:45,  2.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/6_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 22%|██▏       | 6/27 [00:13<00:55,  2.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/7_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 26%|██▌       | 7/27 [00:15<00:51,  2.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/8_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 30%|██▉       | 8/27 [00:17<00:44,  2.32s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/9_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 33%|███▎      | 9/27 [00:19<00:40,  2.24s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/10_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 37%|███▋      | 10/27 [00:21<00:38,  2.25s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/11_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 41%|████      | 11/27 [00:23<00:33,  2.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/12_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 44%|████▍     | 12/27 [00:25<00:30,  2.06s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/13_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 48%|████▊     | 13/27 [00:27<00:29,  2.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/14_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 52%|█████▏    | 14/27 [00:30<00:30,  2.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/15_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 56%|█████▌    | 15/27 [00:32<00:24,  2.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/16_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 59%|█████▉    | 16/27 [00:33<00:20,  1.88s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/17_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 63%|██████▎   | 17/27 [00:35<00:17,  1.78s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/18_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 67%|██████▋   | 18/27 [00:36<00:15,  1.69s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/19_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 70%|███████   | 19/27 [00:38<00:13,  1.68s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/20_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 74%|███████▍  | 20/27 [00:39<00:11,  1.60s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/21_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 78%|███████▊  | 21/27 [00:41<00:09,  1.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/22_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 81%|████████▏ | 22/27 [00:42<00:07,  1.60s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/23_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 85%|████████▌ | 23/27 [00:44<00:06,  1.60s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/24_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 89%|████████▉ | 24/27 [00:46<00:04,  1.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/25_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 93%|█████████▎| 25/27 [00:47<00:03,  1.60s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/26_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 96%|█████████▋| 26/27 [00:49<00:01,  1.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Train/27_train_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 27/27 [00:50<00:00,  1.87s/it]"],"name":"stderr"},{"output_type":"stream","text":["Number of files wrote to TFRecords: 802\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"2qy_SRGXBKda"},"source":["#### Create validation TFRecords"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"JQiLA5pkBKdb","outputId":"dc340f3b-17fd-4a3c-c5f8-a11f2adbf5f6"},"source":["write_images_to_tfr(val_dataset, val_labels, \n","                    max_files = 30, \n","                    filename = 'val_volumes',\n","                    out_dir = './Datasets/TFRecords/Validation/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/6 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["[+] Number of TFRecords needed for 172 volumes: 6\n","    [-] Number of files per TFRecord: 30\n","\n","[+] Output directory: ../Datasets/TFRecords/Validation/\n","\n","[+] Writing TFRecord: ../Datasets/TFRecords/Validation/1_val_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 17%|█▋        | 1/6 [00:01<00:06,  1.24s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Validation/2_val_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 33%|███▎      | 2/6 [00:02<00:05,  1.40s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Validation/3_val_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 50%|█████     | 3/6 [00:04<00:04,  1.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Validation/4_val_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 67%|██████▋   | 4/6 [00:06<00:03,  1.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Validation/5_val_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 83%|████████▎ | 5/6 [00:07<00:01,  1.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Validation/6_val_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 6/6 [00:08<00:00,  1.50s/it]"],"name":"stderr"},{"output_type":"stream","text":["Number of files wrote to TFRecords: 172\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"exqrKsF6BKdc"},"source":["#### Create testing TFRecords"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ViV6lKTkBKdd","outputId":"5ec29e2b-330e-4a72-864c-b715ffeb2329"},"source":["write_images_to_tfr(test_dataset, test_labels, \n","                    max_files = 30, \n","                    filename = 'test_volumes',\n","                    out_dir = './Datasets/TFRecords/Test/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/6 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["[+] Number of TFRecords needed for 172 volumes: 6\n","    [-] Number of files per TFRecord: 30\n","\n","[+] Output directory: ../Datasets/TFRecords/Test/\n","\n","[+] Writing TFRecord: ../Datasets/TFRecords/Test/1_test_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 17%|█▋        | 1/6 [00:01<00:07,  1.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Test/2_test_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 33%|███▎      | 2/6 [00:03<00:06,  1.52s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Test/3_test_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 50%|█████     | 3/6 [00:04<00:04,  1.52s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Test/4_test_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 67%|██████▋   | 4/6 [00:06<00:03,  1.66s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Test/5_test_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["\r 83%|████████▎ | 5/6 [00:08<00:01,  1.84s/it]"],"name":"stderr"},{"output_type":"stream","text":["[+] Writing TFRecord: ../Datasets/TFRecords/Test/6_test_volumes.tfrecords\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 6/6 [00:09<00:00,  1.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["Number of files wrote to TFRecords: 172\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"U2GYKSG2BKde"},"source":[""],"execution_count":null,"outputs":[]}]}