{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFR preprocessing\n",
    "The goal of this Notebook is to convert all the 3D images that have been preprocessed and saved as numpy files in the previous Notebook (2_MRI_preprocessing) to TFRecords.\n",
    "\n",
    "According to [Tensorflow](https://www.tensorflow.org/tutorials/load_data/tfrecord), TFRecords are used to store the data as a sequence of binary strings. The main advantage of using TFRecords is that it speeds up data reading.\n",
    "\n",
    "This notebook is structured as follows:\n",
    "   - Initial setup\n",
    "   - Import libraries\n",
    "   - Load features\n",
    "   - Load labels\n",
    "   - Create TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify if user is working on Google Drive\n",
    "google_drive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if google_drive == True:\n",
    "    \n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    path = \"./drive/MyDrive/TFM/Code/\"\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append(path)\n",
    "\n",
    "else:\n",
    "    path = \"../\"\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load 3D images filenames (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify folders where there are the 3D images in numpy’s compressed format (.npz)\n",
    "root_3d_volumes = path + \"Datasets/Volume_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of 3D volumes: 1085\n"
     ]
    }
   ],
   "source": [
    "# Get list of filenames from 3D volumes\n",
    "filenames = os.listdir(root_3d_volumes)\n",
    "\n",
    "# Remove \".DS_Store\" file from list\n",
    "if \".DS_Store\" in filenames:\n",
    "    filenames.remove(\".DS_Store\")\n",
    "\n",
    "# Include all the path for each file\n",
    "filenames = [path + \"Datasets/Volume_files/\" + file for file in filenames]\n",
    "\n",
    "# Check number of images loaded\n",
    "print(\"[+] Number of 3D volumes:\", len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle list of filenames\n",
    "random.shuffle(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset into training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.7\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames, test_filenames = train_test_split(filenames, \n",
    "                                               test_size = 0.3, \n",
    "                                               random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames, val_filenames = train_test_split(test_filenames, \n",
    "                                             test_size = 0.5, \n",
    "                                             random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Training size: 759\n",
      "[+] Validation size: 163\n",
      "[+] Testing size: 163\n"
     ]
    }
   ],
   "source": [
    "# Check size of each dataset\n",
    "print(f\"[+] Training size:\", len(train_filenames))\n",
    "print(f\"[+] Validation size:\", len(val_filenames))\n",
    "print(f\"[+] Testing size:\", len(test_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load array for each 3D volume (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_volumes(filenames):\n",
    "\n",
    "    # List where to save the 3D volumes\n",
    "    volumes = []\n",
    "\n",
    "    # Load 3D images from filenames list\n",
    "    for file in filenames:\n",
    "\n",
    "        # Read 3D image\n",
    "        volume = np.load(file, allow_pickle= True)['arr_0']\n",
    "        volume = volume[10:120, 30:160, 15:95]\n",
    "\n",
    "        # Append 3D image to volumes list \n",
    "        volumes.append(volume)\n",
    "        \n",
    "        if len(volumes) % 100 == 0:\n",
    "            print(f\"{len(volumes)} loaded\")\n",
    "    return np.array(volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loaded\n",
      "200 loaded\n",
      "300 loaded\n",
      "400 loaded\n",
      "500 loaded\n",
      "600 loaded\n",
      "700 loaded\n"
     ]
    }
   ],
   "source": [
    "# Load training arrays\n",
    "train_dataset = return_volumes(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation arrays\n",
    "val_dataset = return_volumes(val_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing arrays\n",
    "test_dataset = return_volumes(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Training shape: (759, 110, 130, 80)\n",
      "[+] Validation shape: (163, 110, 130, 80)\n",
      "[+] Testing shape: (163, 110, 130, 80)\n"
     ]
    }
   ],
   "source": [
    "# Check size of each dataset\n",
    "print(f\"[+] Training shape:\", train_dataset.shape)\n",
    "print(f\"[+] Validation shape:\", val_dataset.shape)\n",
    "print(f\"[+] Testing shape:\", test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CSV files with image details: images IDs and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageDataID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>AcqDate</th>\n",
       "      <th>Format</th>\n",
       "      <th>Downloaded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I125941</td>\n",
       "      <td>137_S_1426</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>10/30/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I121703</td>\n",
       "      <td>128_S_1408</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/19/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I121637</td>\n",
       "      <td>037_S_1421</td>\n",
       "      <td>MCI</td>\n",
       "      <td>F</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/17/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I122382</td>\n",
       "      <td>128_S_1407</td>\n",
       "      <td>MCI</td>\n",
       "      <td>F</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/05/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I121689</td>\n",
       "      <td>127_S_1427</td>\n",
       "      <td>MCI</td>\n",
       "      <td>F</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/02/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ImageDataID     Subject Group Sex  Age  Visit Modality  \\\n",
       "0     I125941  137_S_1426   MCI   M   85      4      MRI   \n",
       "1     I121703  128_S_1408   MCI   M   73      4      MRI   \n",
       "2     I121637  037_S_1421   MCI   F   76      4      MRI   \n",
       "3     I122382  128_S_1407   MCI   F   76      4      MRI   \n",
       "4     I121689  127_S_1427   MCI   F   71      4      MRI   \n",
       "\n",
       "                                Description       Type     AcqDate Format  \\\n",
       "0               MPR-R; GradWarp; N3; Scaled  Processed  10/30/2008  NiFTI   \n",
       "1  MPR; GradWarp; B1 Correction; N3; Scaled  Processed   9/19/2008  NiFTI   \n",
       "2                 MPR; GradWarp; N3; Scaled  Processed   9/17/2008  NiFTI   \n",
       "3  MPR; GradWarp; B1 Correction; N3; Scaled  Processed   9/05/2008  NiFTI   \n",
       "4  MPR; GradWarp; B1 Correction; N3; Scaled  Processed   9/02/2008  NiFTI   \n",
       "\n",
       "   Downloaded  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load individuals CSV files with image details\n",
    "df_1 = pd.read_csv(path + \"Datasets/ADNI1_Complete_1Yr_1.5T.csv\")\n",
    "df_2 = pd.read_csv(path + \"Datasets/ADNI1_Complete_2Yr_1.5T.csv\")\n",
    "df_3 = pd.read_csv(path + \"Datasets/ADNI1_Complete_3Yr_1.5T.csv\")\n",
    "\n",
    "# Concatenate all CSV files in a unique dataframe\n",
    "df = pd.concat([df_1, df_2, df_3])\n",
    "\n",
    "# Remove extra whitespaces from column names\n",
    "df.columns = df.columns.str.replace(\" \", \"\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageDataID</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I125941</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I121703</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I121637</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I122382</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I121689</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ImageDataID Group\n",
       "0     I125941   MCI\n",
       "1     I121703   MCI\n",
       "2     I121637   MCI\n",
       "3     I122382   MCI\n",
       "4     I121689   MCI"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve only the image ID and Group (class) columns\n",
    "df = df[[\"ImageDataID\", \"Group\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCI    1708\n",
       "CN     1009\n",
       "AD      575\n",
       "Name: Group, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of cases by class in the dataframe\n",
    "df[\"Group\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load class for each 3D volume (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_labels(filenames, df):\n",
    "\n",
    "    # List where to save the class for each image: 0 (CN), 1 (AD, MCI)\n",
    "    labels = []\n",
    "\n",
    "    # Load labels from filenames list\n",
    "    for file in filenames:\n",
    "\n",
    "        # Get image ID from file name\n",
    "        image_id = file.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        # Retireve class from dataframe searching by image ID\n",
    "        label = df[\"Group\"].loc[df['ImageDataID'] == image_id].values[0]\n",
    "\n",
    "        # Assign class numerica values depending the group: 0 (CN) or 1 (AD)\n",
    "        if label in [\"CN\"]:\n",
    "            labels.append([0]) \n",
    "        elif label in [\"AD\"]:\n",
    "            labels.append([1])  \n",
    "        else:\n",
    "            print(f\"ERROR with image ID {image_id}\")\n",
    "    \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training labels\n",
    "train_labels = return_labels(train_filenames, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation labels\n",
    "val_labels = return_labels(val_filenames, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing labels\n",
    "test_labels = return_labels(test_filenames, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Training labels shape: (759, 1)\n",
      "[+] Validation labels shape: (163, 1)\n",
      "[+] Testing labels shape: (163, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check size of each dataset\n",
    "print(f\"[+] Training labels shape:\", train_labels.shape)\n",
    "print(f\"[+] Validation labels shape:\", val_labels.shape)\n",
    "print(f\"[+] Testing labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/a-practical-guide-to-tfrecords-584536bc786c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    \n",
    "    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    "    \n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a floast_list from a float / double.\"\"\"\n",
    "    \n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    \n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_array(array):\n",
    "    \n",
    "    array = tf.io.serialize_tensor(array)\n",
    "    \n",
    "    return array\n",
    "\n",
    "def parse_single_volume(volume, label):\n",
    "    \n",
    "    # Get first value of label, as it is an array of length 1\n",
    "    label = label[0]\n",
    "    \n",
    "    # Define the dictionary -- the structure -- of our single example\n",
    "    data = {'height' : _int64_feature(volume.shape[0]),\n",
    "            'width' : _int64_feature(volume.shape[1]),\n",
    "            'depth' : _int64_feature(volume.shape[2]),\n",
    "            'raw_image' : _bytes_feature(serialize_array(volume)),\n",
    "            'label' : _int64_feature(label)}\n",
    "    \n",
    "    # Create an Example, wrapping the single features\n",
    "    out = tf.train.Example(features = tf.train.Features(feature = data))\n",
    "\n",
    "    return out\n",
    "\n",
    "def write_images_to_tfr(volumes, labels, filename = \"images\", max_files = 10, out_dir = \"../Datasets/\"):\n",
    "\n",
    "    # Determine the number of TFRecords needed\n",
    "    splits = (len(volumes)//max_files) + 1 \n",
    "    if len(volumes) % max_files == 0:\n",
    "        splits-=1   \n",
    "    \n",
    "    print(f\"[+] Number of TFRecords needed for {len(volumes)} volumes: {splits}\")\n",
    "    print(f\"    [-] Number of files per TFRecord: {max_files}\")\n",
    "    \n",
    "    # Check directory\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)   \n",
    "    print(f\"\\n[+] Output directory: {out_dir}\\n\")\n",
    "    \n",
    "    # Write TFRecords\n",
    "    file_count = 0\n",
    "    \n",
    "    for i in tqdm.tqdm(range(splits)):\n",
    "        \n",
    "        # Retrieve name of the TFRecord\n",
    "        tfr_name = \"{}{}_{}.tfrecords\".format(out_dir, i+1, filename)\n",
    "        print(f\"[+] Writing TFRecord: {tfr_name}\")\n",
    "\n",
    "        # Start writer\n",
    "        writer = tf.io.TFRecordWriter(tfr_name)\n",
    "        current_tfr_count = 0\n",
    "    \n",
    "        while current_tfr_count < max_files: \n",
    "            \n",
    "            # Get the index of the file that we want to parse now\n",
    "            index = i * max_files + current_tfr_count\n",
    "            \n",
    "            # Check if all dataset has been added to TFRecords\n",
    "            if index == len(volumes):\n",
    "                break\n",
    "                \n",
    "            # Retrieve volume and label\n",
    "            current_volume = volumes[index]\n",
    "            current_label = labels[index]\n",
    "\n",
    "            # Create the required example representation\n",
    "            out = parse_single_volume(volume = current_volume, label = current_label)\n",
    "\n",
    "            writer.write(out.SerializeToString())\n",
    "            \n",
    "            # Update counters\n",
    "            current_tfr_count+=1\n",
    "            file_count += 1\n",
    "       \n",
    "        # Close writer\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"[+] Number of files wrote to TFRecords: {file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of TFRecords needed for 759 volumes: 26\n",
      "    [-] Number of files per TFRecord: 30\n",
      "\n",
      "[+] Output directory: ../Datasets/TFRecords/Train/\n",
      "\n",
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/1_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/26 [00:01<00:31,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/2_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/26 [00:02<00:29,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/3_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/26 [00:03<00:27,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/4_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 4/26 [00:04<00:26,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/5_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 5/26 [00:06<00:27,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/6_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 6/26 [00:07<00:27,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/7_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 7/26 [00:09<00:28,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/8_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 8/26 [00:11<00:26,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/9_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▍      | 9/26 [00:12<00:25,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/10_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 10/26 [00:14<00:23,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/11_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 11/26 [00:15<00:21,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/12_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 12/26 [00:16<00:18,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/13_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 13/26 [00:17<00:17,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/14_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 14/26 [00:18<00:15,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/15_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 15/26 [00:20<00:14,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/16_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 16/26 [00:21<00:12,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/17_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 17/26 [00:22<00:11,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/18_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 18/26 [00:23<00:09,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/19_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 19/26 [00:25<00:08,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/20_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 20/26 [00:26<00:07,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/21_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 21/26 [00:27<00:06,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/22_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 22/26 [00:28<00:05,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/23_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 23/26 [00:30<00:03,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/24_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 24/26 [00:31<00:02,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/25_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 25/26 [00:32<00:01,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Train/26_train_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:33<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of files wrote to TFRecords: 759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_images_to_tfr(train_dataset, train_labels, \n",
    "                    max_files = 30, \n",
    "                    filename = \"train_volumes\",\n",
    "                    out_dir = \"../Datasets/TFRecords/Train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create validation TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of TFRecords needed for 163 volumes: 6\n",
      "    [-] Number of files per TFRecord: 30\n",
      "\n",
      "[+] Output directory: ../Datasets/TFRecords/Validation/\n",
      "\n",
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/1_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 1/6 [00:01<00:07,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/2_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:02<00:06,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/3_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:04<00:04,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/4_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:06<00:03,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/5_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:07<00:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Validation/6_val_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of files wrote to TFRecords: 163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_images_to_tfr(val_dataset, val_labels, \n",
    "                    max_files = 30, \n",
    "                    filename = \"val_volumes\",\n",
    "                    out_dir = \"../Datasets/TFRecords/Validation/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create test TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of TFRecords needed for 163 volumes: 6\n",
      "    [-] Number of files per TFRecord: 30\n",
      "\n",
      "[+] Output directory: ../Datasets/TFRecords/Test/\n",
      "\n",
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/1_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 1/6 [00:01<00:09,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/2_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:03<00:06,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/3_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:04<00:04,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/4_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:05<00:03,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/5_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:07<00:01,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Writing TFRecord: ../Datasets/TFRecords/Test/6_test_volumes.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of files wrote to TFRecords: 163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_images_to_tfr(test_dataset, test_labels, \n",
    "                    max_files = 30, \n",
    "                    filename = \"test_volumes\",\n",
    "                    out_dir = \"../Datasets/TFRecords/Test/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
